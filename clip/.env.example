# presto-ai-cli Environment Variables
# Copy this to .env and fill in your values
# Or export these in your shell profile (.bashrc, .zshrc, etc.)

# =============================================================================
# LLM Provider API Keys (set one based on your provider)
# =============================================================================

# OpenAI
OPENAI_API_KEY=sk-your-openai-api-key-here

# Anthropic Claude
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# For custom endpoints (Azure OpenAI, vLLM, etc.)
# Set via: presto-ai config set llm_api_base https://your-endpoint.com/v1

# =============================================================================
# Apache Superset (Optional)
# =============================================================================

# Superset base URL
# SUPERSET_URL=https://superset.your-company.com

# Authentication (use either username/password OR token)
# SUPERSET_USERNAME=your-username
# SUPERSET_PASSWORD=your-password

# Or use API token instead
# SUPERSET_TOKEN=your-superset-api-token

# =============================================================================
# Presto Connection (can also be set via presto-ai config)
# =============================================================================

# These are typically set via the CLI config command, but can also be
# passed as environment variables if preferred:
#
# PRESTO_HOST=your-presto-coordinator.com
# PRESTO_PORT=8080
# PRESTO_USER=your-username
# PRESTO_CATALOG=hive
# PRESTO_SCHEMA=default

# =============================================================================
# Optional: Ollama (for local models)
# =============================================================================

# If running Ollama on a non-default host
# OLLAMA_HOST=http://localhost:11434
